# -*- coding: utf-8 -*-
"""Theoretical-Statistics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lJOk0KEJq1JCcF7qh_o-iHvOb3CNTBMN
"""

import pandas as pd
import scipy as sp
import numpy as np
import datetime as dt
import os

import matplotlib.pyplot as plt
import seaborn as sb
from scipy import stats

df = pd.read_csv(
    "April 2025 Raw Monthly Ridership (no adjustments or estimates)_250602.csv")

# Drop rows where 'Mode/Type of Service Status' is 'Inactive'
initial_rows = len(df)
df = df[df['Mode/Type of Service Status'] != 'Inactive']
df.drop(labels=['Legacy NTD ID','Last Closed Report Year', 'UACE CD','Reporter Type','Mode/Type of Service Status'], axis = 1, inplace=True)
print("\nNumber of NaN values per column:")
print(df.isnull().sum())
df.dropna(axis=0, inplace=True)
rows_dropped = initial_rows - len(df)

print(f"\nDropped {rows_dropped} rows where 'Mode/Type of Service Status' was 'Inactive' and \nRows left {len(df)}.")
print("\nDataFrame after dropping rows and Nans:")
df

modes = df['Mode'].unique()
mode_dfs = {}
for mode in modes:
  mode_dfs[mode] = df[df['Mode'] == mode].copy()

# You can access each dataframe like this:
for mode, mode_df in mode_dfs.items():
   print(f"\nDataFrame for Mode: {mode}")
   print(mode_df.head())

# prompt: Now, can you please show me the size of each mode dataset?

for mode, mode_df in mode_dfs.items():
  print(f"\nSize of DataFrame for Mode {mode}: {len(mode_df)}")

# prompt: Can you please do the same for the column "3 Mode" ?

mode3_modes = df['3 Mode'].unique()
mode3_mode_dfs = {}
for mode3_mode in mode3_modes:
  mode3_mode_dfs[mode3_mode] = df[df['3 Mode'] == mode3_mode].copy()

# You can access each dataframe like this:
for mode3_mode, mode3_mode_df in mode3_mode_dfs.items():
   print(f"\nDataFrame for 3 Mode: {mode3_mode}")
   print(mode3_mode_df.head())

for mode3_mode, mode3_mode_df in mode3_mode_dfs.items():
  print(f"\nSize of DataFrame for 3 Mode {mode3_mode}: {len(mode3_mode_df)}")

# prompt: Use the column "NTD ID" as the key, and for every time there are several "3 Mode" identical values for the same key, take the one with the higher "Unlinked Passenger Trips FY".
# Put the clean results in "One-df" dataframe

# Sort the DataFrame by 'NTD ID' and 'Unlinked Passenger Trips FY' in descending order
df_sorted = df.sort_values(by=['NTD ID', 'Unlinked Passenger Trips FY'], ascending=[True, False])

# Drop duplicate rows based on 'NTD ID' and '3 Mode', keeping the first occurrence (which has the highest 'Unlinked Passenger Trips FY' due to sorting)
One_df = df_sorted.drop_duplicates(subset=['NTD ID', '3 Mode'], keep='first').copy()

print("\nDataFrame after selecting the '3 Mode' with the highest 'Unlinked Passenger Trips FY' for each 'NTD ID':")
print(One_df.head())
print(f"\nSize of the One_df: {len(One_df)}")

# prompt: Can you please show me how many unique entries exist for each "3 Mode" values? Use One-df

print("\nNumber of unique entries for each '3 Mode' value in One_df:")
print(One_df['3 Mode'].value_counts())

# prompt: Can you please add a "Ratio" value for each entry, where the value is calculated by "Unlinked Passenger Trips FY"/ " Service Area Population " Use One-df. turn them numeric first, but try to do it in a way where they wont all be NaN. Don't use errors='coerce'
# Please change the column name from " Service Area Population " to "Service Area Population" first

# Function to convert strings to numeric, handling various non-numeric characters
def safe_numeric_conversion(value):
  if isinstance(value, (int, float)):
    return value
  try:
    # Attempt to remove common non-numeric characters like commas, dollar signs, spaces
    cleaned_value = str(value).replace(',', '').replace('$', '').strip()
    # Check if the cleaned value is empty after stripping
    if not cleaned_value:
      return np.nan
    return pd.to_numeric(cleaned_value)
  except ValueError:
    return np.nan

# Rename the column
One_df.rename(columns={' Service Area Population ': 'Service Area Population'}, inplace=True)

# Apply the safe conversion function to the relevant columns
One_df['Unlinked Passenger Trips FY_numeric'] = One_df['Unlinked Passenger Trips FY'].apply(safe_numeric_conversion)
One_df['Service Area Population_numeric'] = One_df['Service Area Population'].apply(safe_numeric_conversion)

# Calculate the "Ratio"
# Use .loc to avoid SettingWithCopyWarning
One_df.loc[:, 'Ratio'] = One_df['Unlinked Passenger Trips FY_numeric'] / One_df['Service Area Population_numeric']

# Display the first few rows with the new 'Ratio' column
print("\nDataFrame with 'Ratio' column:")
print(One_df[['Unlinked Passenger Trips FY', 'Service Area Population', 'Unlinked Passenger Trips FY_numeric', 'Service Area Population_numeric', 'Ratio']].head())

# prompt: Now, can you please plot these ratios according to each unique value in "3 Mode" ? Agency Name is not defined. Don't use mean, but plot the values. Plot another set where outliers are ignored - cut the maximum value. Use histograms

# Get the unique values from the "3 Mode" column
three_modes = One_df['3 Mode'].unique()

# Create histograms for each unique "3 Mode" value
for mode in three_modes:
  # Filter the DataFrame for the current "3 Mode"
  mode_df = One_df[One_df['3 Mode'] == mode]

  # Create the histogram
  plt.figure(figsize=(10, 6))
  plt.hist(mode_df['Ratio'].dropna(), bins=20) # Use dropna to handle potential NaNs in Ratio
  plt.title(f'Histogram of Ratios for 3 Mode: {mode}')
  plt.xlabel('Ratio (Unlinked Passenger Trips FY / Service Area Population)')
  plt.ylabel('Frequency')
  plt.grid(True)
  plt.show()

# Create histograms for each unique "3 Mode" value, ignoring outliers
for mode in three_modes:
  # Filter the DataFrame for the current "3 Mode"
  mode_df = One_df[One_df['3 Mode'] == mode].copy() # Use .copy() to avoid SettingWithCopyWarning

  # Calculate the 99th percentile to use as a cutoff for outliers
  ratio_values = mode_df['Ratio'].dropna()
  if not ratio_values.empty:
    cutoff = ratio_values.quantile(0.99)

    # Filter out values above the cutoff
    mode_df_filtered = mode_df[mode_df['Ratio'] <= cutoff]

    # Create the histogram
    plt.figure(figsize=(10, 6))
    plt.hist(mode_df_filtered['Ratio'].dropna(), bins=20) # Use dropna
    plt.title(f'Histogram of Ratios (Outliers Removed) for 3 Mode: {mode}')
    plt.xlabel('Ratio (Unlinked Passenger Trips FY / Service Area Population)')
    plt.ylabel('Frequency')
    plt.grid(True)
    plt.show()
  else:
    print(f"No valid ratio data for 3 Mode: {mode} after dropping NaNs.")

# prompt: Now, can you please create a different analysis about the "Organization Type"?

# Assuming 'df' still contains the original data before dropping 'Organization Type'
# If 'df' was modified, you might need to reload the data or adjust the code accordingly.

# Reload or ensure the original df is available with 'Organization Type'
# If df was already loaded and filtered, make sure you have access to the version
# with the 'Organization Type' column before it was dropped.
# For demonstration, let's assume df_original is the dataframe before dropping the column.
# If you dropped it earlier, you'll need to load the CSV again or use a copy
# made before the drop operation.

# Example: Assuming you still have the original dataframe loaded
# If you don't, uncomment and run the following lines to reload:
# df_original = pd.read_csv("April 2025 Raw Monthly Ridership (no adjustments or estimates)_250602.csv")
# df_original = df_original[df_original['Mode/Type of Service Status'] != 'Inactive']

# Select the columns relevant for this analysis
organization_analysis_df = df[['NTD ID', 'Organization Type', 'Unlinked Passenger Trips FY']].copy()

# Clean 'Unlinked Passenger Trips FY' column
organization_analysis_df['Unlinked Passenger Trips FY_numeric'] = organization_analysis_df['Unlinked Passenger Trips FY'].apply(safe_numeric_conversion)

# Drop rows with NaN in the numeric trips column
organization_analysis_df.dropna(subset=['Unlinked Passenger Trips FY_numeric'], inplace=True)

# Group by 'Organization Type' and calculate the total 'Unlinked Passenger Trips FY'
organization_trips = organization_analysis_df.groupby('Organization Type')['Unlinked Passenger Trips FY_numeric'].sum().reset_index()

# Sort the results by total trips in descending order
organization_trips_sorted = organization_trips.sort_values(by='Unlinked Passenger Trips FY_numeric', ascending=False)

print("\nTotal Unlinked Passenger Trips FY by Organization Type:")
print(organization_trips_sorted)

# Create a bar plot of total unlinked passenger trips by organization type
plt.figure(figsize=(12, 7))
sb.barplot(x='Unlinked Passenger Trips FY_numeric', y='Organization Type', data=organization_trips_sorted, palette='viridis')
plt.title('Total Unlinked Passenger Trips FY by Organization Type')
plt.xlabel('Total Unlinked Passenger Trips FY')
plt.ylabel('Organization Type')
plt.tight_layout()
plt.show()

# Analyze the count of agencies per organization type
organization_counts = organization_analysis_df.groupby('Organization Type')['NTD ID'].nunique().reset_index()
organization_counts_sorted = organization_counts.sort_values(by='NTD ID', ascending=False)

print("\nNumber of Unique Agencies by Organization Type:")
print(organization_counts_sorted)

# Create a bar plot of the number of agencies by organization type
plt.figure(figsize=(12, 7))
sb.barplot(x='NTD ID', y='Organization Type', data=organization_counts_sorted, palette='viridis')
plt.title('Number of Unique Agencies by Organization Type')
plt.xlabel('Number of Unique Agencies')
plt.ylabel('Organization Type')
plt.tight_layout()
plt.show()

# You could also calculate the average trips per agency within each organization type
organization_analysis_df['NTD ID'] = organization_analysis_df['NTD ID'].astype(str) # Ensure NTD ID is string for unique counting

avg_trips_per_agency = organization_analysis_df.groupby('Organization Type').agg(
    total_trips=('Unlinked Passenger Trips FY_numeric', 'sum'),
    num_agencies=('NTD ID', 'nunique')
).reset_index()

avg_trips_per_agency['Average Trips per Agency'] = avg_trips_per_agency['total_trips'] / avg_trips_per_agency['num_agencies']
avg_trips_per_agency_sorted = avg_trips_per_agency.sort_values(by='Average Trips per Agency', ascending=False)

print("\nAverage Unlinked Passenger Trips FY per Agency by Organization Type:")
print(avg_trips_per_agency_sorted[['Organization Type', 'Average Trips per Agency']])

# Create a bar plot for average trips per agency
plt.figure(figsize=(12, 7))
sb.barplot(x='Average Trips per Agency', y='Organization Type', data=avg_trips_per_agency_sorted, palette='viridis')
plt.title('Average Unlinked Passenger Trips FY per Agency by Organization Type')
plt.xlabel('Average Unlinked Passenger Trips FY per Agency')
plt.ylabel('Organization Type')
plt.tight_layout()
plt.show()

# prompt: Can you please make 2 categories, where the first is public and the second is private? I want to simplify these org types.

# Define mappings for simplifying organization types
public_types = [
    'Independent Public Agency or Authority of Transit Service',
    'City, County or Local Government Unit or Department of Transportation',
    'State Government Unit or Department of Transportation',
    'MPO, COG or Other Planning Agency',
    'State Government',
    'State DOT',
    'Special Transit District',
    'Tribe',
    'University',
    'Area Agency on Aging',
    'Other Publicly-Owned or Privately Chartered Corporation',
    'Public agency or authority that contracts for some or all transit service (not a State DOT)',
    'Subsidiary Unit of a Transit Agency, Reporting Separately'

]

private_types = [
    'Private-Non-Profit Corporation',
    'Non-Profit Private',
    'Private-For-Profit Corporation',
    'Private transportation provider reporting on behalf of a public agency or authority (not a broker)',
    'Private Provider Reporting on Behalf of a Public Entity',

]

# Create a new column 'Org Category'
def categorize_organization(org_type):
    if org_type in public_types:
        return 'Public'
    elif org_type in private_types:
        return 'Private'
    else:
        return 'Other' # For any types not explicitly listed

organization_analysis_df['Org Category'] = organization_analysis_df['Organization Type'].apply(categorize_organization)

print("\nDataFrame with 'Org Category':")
print(organization_analysis_df[['Organization Type', 'Org Category']].head())

# Now you can analyze by 'Org Category' instead of 'Organization Type'

# Group by 'Org Category' and calculate the total 'Unlinked Passenger Trips FY'
category_trips = organization_analysis_df.groupby('Org Category')['Unlinked Passenger Trips FY_numeric'].sum().reset_index()

# Sort the results by total trips in descending order
category_trips_sorted = category_trips.sort_values(by='Unlinked Passenger Trips FY_numeric', ascending=False)

print("\nTotal Unlinked Passenger Trips FY by Org Category:")
print(category_trips_sorted)

# Create a bar plot of total unlinked passenger trips by organization category
plt.figure(figsize=(10, 6))
sb.barplot(x='Unlinked Passenger Trips FY_numeric', y='Org Category', data=category_trips_sorted, palette='viridis')
plt.title('Total Unlinked Passenger Trips FY by Organization Category')
plt.xlabel('Total Unlinked Passenger Trips FY')
plt.ylabel('Organization Category')
plt.tight_layout()
plt.show()

# Analyze the count of agencies per organization category
category_counts = organization_analysis_df.groupby('Org Category')['NTD ID'].nunique().reset_index()
category_counts_sorted = category_counts.sort_values(by='NTD ID', ascending=False)

print("\nNumber of Unique Agencies by Org Category:")
print(category_counts_sorted)

# Create a bar plot of the number of agencies by organization category
plt.figure(figsize=(10, 6))
sb.barplot(x='NTD ID', y='Org Category', data=category_counts_sorted, palette='viridis')
plt.title('Number of Unique Agencies by Org Category')
plt.xlabel('Number of Unique Agencies')
plt.ylabel('Organization Category')
plt.tight_layout()
plt.show()

# Calculate the average trips per agency within each organization category
avg_trips_per_agency_category = organization_analysis_df.groupby('Org Category').agg(
    total_trips=('Unlinked Passenger Trips FY_numeric', 'sum'),
    num_agencies=('NTD ID', 'nunique')
).reset_index()

avg_trips_per_agency_category['Average Trips per Agency'] = avg_trips_per_agency_category['total_trips'] / avg_trips_per_agency_category['num_agencies']
avg_trips_per_agency_category_sorted = avg_trips_per_agency_category.sort_values(by='Average Trips per Agency', ascending=False)

print("\nAverage Unlinked Passenger Trips FY per Agency by Org Category:")
print(avg_trips_per_agency_category_sorted[['Org Category', 'Average Trips per Agency']])

# Create a bar plot for average trips per agency by category
plt.figure(figsize=(10, 6))
sb.barplot(x='Average Trips per Agency', y='Org Category', data=avg_trips_per_agency_category_sorted, palette='viridis')
plt.title('Average Unlinked Passenger Trips FY per Agency by Org Category')
plt.xlabel('Average Unlinked Passenger Trips FY per Agency')
plt.ylabel('Organization Category')
plt.tight_layout()
plt.show()

# prompt: print all columns in organization_analysis_df

organization_analysis_df.columns

# prompt: add the value Service Area Population to organization_analysis_df

# Select the columns relevant for this analysis, including 'Service Area Population'
organization_analysis_df = df[['NTD ID', 'Organization Type', 'Unlinked Passenger Trips FY', ' Service Area Population ']].copy()

# Clean 'Unlinked Passenger Trips FY' and 'Service Area Population' columns
organization_analysis_df['Unlinked Passenger Trips FY_numeric'] = organization_analysis_df['Unlinked Passenger Trips FY'].apply(safe_numeric_conversion)
organization_analysis_df['Service Area Population_numeric'] = organization_analysis_df[' Service Area Population '].apply(safe_numeric_conversion)

# Drop rows with NaN in the numeric trips column or service area population
organization_analysis_df.dropna(subset=['Unlinked Passenger Trips FY_numeric', 'Service Area Population_numeric'], inplace=True)

print("\nDataFrame with 'Service Area Population' added and cleaned:")
print(organization_analysis_df[['NTD ID', 'Organization Type', 'Unlinked Passenger Trips FY_numeric', 'Service Area Population_numeric']].head())

# prompt: Now, can you compare them after normalising using Service Area Population?
# KeyError                                  Traceback (most recent call last)
# /tmp/ipython-input-51-3958785882.py in <cell line: 0>()
#      27
#      28 # Group by 'Org Category' and calculate the average normalized trips
# ---> 29 category_avg_normalized_trips = organization_analysis_df.groupby('Org Category')['Trips per Capita'].mean().reset_index()
#      30
#      31 # Sort the results by average normalized trips in descending order
# 2 frames
# /usr/local/lib/python3.11/dist-packages/pandas/core/groupby/grouper.py in get_grouper(obj, key, axis, level, sort, observed, validate, dropna)
#    1041                 in_axis, level, gpr = False, gpr, None
#    1042             else:
# -> 1043                 raise KeyError(gpr)
#    1044         elif isinstance(gpr, Grouper) and gpr.key is not None:
#    1045             # Add key to exclusions
# KeyError: 'Org Category'

# Calculate 'Trips per Capita' (normalized trips)
organization_analysis_df['Trips per Capita'] = organization_analysis_df['Unlinked Passenger Trips FY_numeric'] / organization_analysis_df['Service Area Population_numeric']

# Create a new column 'Org Category'
def categorize_organization(org_type):
    if org_type in public_types:
        return 'Public'
    elif org_type in private_types:
        return 'Private'
    else:
        return 'Other' # For any types not explicitly listed

organization_analysis_df['Org Category'] = organization_analysis_df['Organization Type'].apply(categorize_organization)

print("\nDataFrame with 'Org Category' and 'Trips per Capita':")
print(organization_analysis_df[['Organization Type', 'Org Category', 'Trips per Capita']].head())


# Group by 'Org Category' and calculate the average normalized trips
category_avg_normalized_trips = organization_analysis_df.groupby('Org Category')['Trips per Capita'].mean().reset_index()

# Sort the results by average normalized trips in descending order
category_avg_normalized_trips_sorted = category_avg_normalized_trips.sort_values(by='Trips per Capita', ascending=False)

print("\nAverage Trips per Capita by Org Category:")
print(category_avg_normalized_trips_sorted)

# Create a bar plot of average normalized trips by organization category
plt.figure(figsize=(10, 6))
sb.barplot(x='Trips per Capita', y='Org Category', data=category_avg_normalized_trips_sorted, palette='viridis')
plt.title('Average Unlinked Passenger Trips FY per Capita by Organization Category')
plt.xlabel('Average Trips per Capita')
plt.ylabel('Organization Category')
plt.tight_layout()
plt.show()

# prompt: Can you please remove outliers above 95%?

# Function to remove outliers above a specified percentile
def remove_outliers_above_percentile(df, column, percentile=95):
    """
    Removes rows from a DataFrame where the values in a specified column
    are above the given percentile.

    Args:
        df (pd.DataFrame): The input DataFrame.
        column (str): The name of the column to check for outliers.
        percentile (int or float): The percentile cutoff (e.g., 95 for 95th percentile).

    Returns:
        pd.DataFrame: The DataFrame with outliers removed.
    """
    # Calculate the cutoff value based on the specified percentile
    cutoff = df[column].quantile(percentile / 100)

    # Filter the DataFrame to keep only rows where the column value is less than or equal to the cutoff
    df_filtered = df[df[column] <= cutoff].copy() # Use .copy() to avoid SettingWithCopyWarning

    print(f"\nRemoved outliers in '{column}' above the {percentile}th percentile (cutoff value: {cutoff:.4f}).")
    print(f"Original number of rows: {len(df)}")
    print(f"Number of rows after removing outliers: {len(df_filtered)}")

    return df_filtered

# Apply the outlier removal function to the 'Trips per Capita' column
# Let's remove outliers above the 95th percentile in the 'Trips per Capita' column
organization_analysis_df_no_outliers = remove_outliers_above_percentile(
    organization_analysis_df, 'Trips per Capita', percentile=95
)

print("\nDataFrame after removing outliers in 'Trips per Capita':")
print(organization_analysis_df_no_outliers[['Organization Type', 'Org Category', 'Trips per Capita']].head())


# Now, repeat the analysis using the DataFrame with outliers removed

# Group by 'Org Category' and calculate the average normalized trips
category_avg_normalized_trips_no_outliers = organization_analysis_df_no_outliers.groupby('Org Category')['Trips per Capita'].mean().reset_index()

# Sort the results by average normalized trips in descending order
category_avg_normalized_trips_no_outliers_sorted = category_avg_normalized_trips_no_outliers.sort_values(by='Trips per Capita', ascending=False)

print("\nAverage Trips per Capita by Org Category (Outliers Removed):")
print(category_avg_normalized_trips_no_outliers_sorted)

# Create a bar plot of average normalized trips by organization category (with outliers removed)
plt.figure(figsize=(10, 6))
sb.barplot(x='Trips per Capita', y='Org Category', data=category_avg_normalized_trips_no_outliers_sorted, palette='viridis')
plt.title('Average Unlinked Passenger Trips FY per Capita by Organization Category (Outliers Removed > 95%)')
plt.xlabel('Average Trips per Capita')
plt.ylabel('Organization Category')
plt.tight_layout()
plt.show()

# prompt: Can you please run statistical tests to verify? use mannwhitneyu

from scipy.stats import mannwhitneyu

# Prepare data for Mann-Whitney U test
# We need the 'Trips per Capita' values for the 'Public' and 'Private' categories
public_trips_per_capita = organization_analysis_df_no_outliers[
    organization_analysis_df_no_outliers['Org Category'] == 'Public'
]['Trips per Capita'].dropna() # Ensure we don't have NaNs

private_trips_per_capita = organization_analysis_df_no_outliers[
    organization_analysis_df_no_outliers['Org Category'] == 'Private'
]['Trips per Capita'].dropna() # Ensure we don't have NaNs

print(f"\nNumber of data points for Public category: {len(public_trips_per_capita)}")
print(f"Number of data points for Private category: {len(private_trips_per_capita)}")

# Check if there are enough data points in each group for the test
if len(public_trips_per_capita) > 0 and len(private_trips_per_capita) > 0:
    # Perform the Mann-Whitney U test
    # The null hypothesis (H0) is that the distributions of 'Trips per Capita'
    # for Public and Private organizations are the same.
    # The alternative hypothesis (H1) is that the distributions are different.
    try:
        u_statistic, p_value = mannwhitneyu(public_trips_per_capita, private_trips_per_capita, alternative='two-sided')

        print(f"\nMann-Whitney U Test Results:")
        print(f"U Statistic: {u_statistic:.4f}")
        print(f"P-value: {p_value:.4f}")

        # Interpret the results
        alpha = 0.05  # Significance level
        if p_value < alpha:
            print(f"The p-value ({p_value:.4f}) is less than the significance level ({alpha}).")
            print("We reject the null hypothesis.")
            print("There is a statistically significant difference in the distribution of 'Trips per Capita' between Public and Private organizations.")
        else:
            print(f"The p-value ({p_value:.4f}) is greater than or equal to the significance level ({alpha}).")
            print("We fail to reject the null hypothesis.")
            print("There is no statistically significant evidence to suggest a difference in the distribution of 'Trips per Capita' between Public and Private organizations.")

    except ValueError as e:
        print(f"\nError performing Mann-Whitney U test: {e}")
        print("This might happen if one of the groups is empty or has very few data points.")

else:
    print("\nCannot perform Mann-Whitney U test. One or both groups (Public/Private) have no or insufficient data after outlier removal.")

# Optionally, you can perform the test on the data *before* outlier removal as well,
# to see how outlier removal affects the result.
print("\n--- Performing Mann-Whitney U Test on data *before* outlier removal ---")

public_trips_per_capita_orig = organization_analysis_df[
    organization_analysis_df['Org Category'] == 'Public'
]['Trips per Capita'].dropna()

private_trips_per_capita_orig = organization_analysis_df[
    organization_analysis_df['Org Category'] == 'Private'
]['Trips per Capita'].dropna()

print(f"\nNumber of data points for Public category (original): {len(public_trips_per_capita_orig)}")
print(f"Number of data points for Private category (original): {len(private_trips_per_capita_orig)}")


if len(public_trips_per_capita_orig) > 0 and len(private_trips_per_capita_orig) > 0:
    try:
        u_statistic_orig, p_value_orig = mannwhitneyu(public_trips_per_capita_orig, private_trips_per_capita_orig, alternative='two-sided')

        print(f"\nMann-Whitney U Test Results (Original Data):")
        print(f"U Statistic: {u_statistic_orig:.4f}")
        print(f"P-value: {p_value_orig:.4f}")

        alpha = 0.05
        if p_value_orig < alpha:
            print(f"The p-value ({p_value_orig:.4f}) is less than the significance level ({alpha}).")
            print("We reject the null hypothesis.")
            print("There is a statistically significant difference in the distribution of 'Trips per Capita' between Public and Private organizations (Original Data).")
        else:
            print(f"The p-value ({p_value_orig:.4f}) is greater than or equal to the significance level ({alpha}).")
            print("We fail to reject the null hypothesis.")
            print("There is no statistically significant evidence to suggest a difference in the distribution of 'Trips per Capita' between Public and Private organizations (Original Data).")
    except ValueError as e:
         print(f"\nError performing Mann-Whitney U test on original data: {e}")
else:
     print("\nCannot perform Mann-Whitney U test on original data. One or both groups (Public/Private) have no or insufficient data.")

# prompt: Can you show which "Organization Type" values arent included in the 2 categories?

# Identify the 'Organization Type' values that are categorized as 'Other'
other_org_types = organization_analysis_df[organization_analysis_df['Org Category'] == 'Other']['Organization Type'].unique()

print("\n'Organization Type' values not included in 'Public' or 'Private' categories:")
for org_type in other_org_types:
    print(f"- {org_type}")

# Optional: See how many agencies and total trips are in these 'Other' categories
other_category_summary = organization_analysis_df[organization_analysis_df['Org Category'] == 'Other'].agg(
    num_agencies=('NTD ID', 'nunique'),
    total_trips=('Unlinked Passenger Trips FY_numeric', 'sum')
)

print("\nSummary for 'Other' Organization Category:")
other_category_summary

# prompt: Can you plot the ridership distribution in each category? and then plot when normalized

# Group by 'Org Category' and calculate the distribution of 'Trips per Capita'
# We can use a box plot or violin plot to show the distribution.

# Box plot to visualize the distribution per category
plt.figure(figsize=(12, 7))
sb.boxplot(x='Trips per Capita', y='Org Category', data=organization_analysis_df, palette='viridis')
plt.title('Distribution of Unlinked Passenger Trips FY per Capita by Organization Category')
plt.xlabel('Trips per Capita')
plt.ylabel('Organization Category')
plt.xscale('log') # Use log scale for better visualization due to skewed data
plt.tight_layout()
plt.show()

# Violin plot can also show the distribution density
plt.figure(figsize=(12, 7))
sb.violinplot(x='Trips per Capita', y='Org Category', data=organization_analysis_df, palette='viridis', inner='quartile')
plt.title('Distribution (Density) of Unlinked Passenger Trips FY per Capita by Organization Category')
plt.xlabel('Trips per Capita')
plt.ylabel('Organization Category')
plt.xscale('log') # Use log scale for better visualization
plt.tight_layout()
plt.show()


# Now plot the distribution using the data *after* removing outliers
print("\n--- Plotting Distribution After Outlier Removal ---")

# Box plot with outliers removed
plt.figure(figsize=(12, 7))
sb.boxplot(x='Trips per Capita', y='Org Category', data=organization_analysis_df_no_outliers, palette='viridis')
plt.title('Distribution of Unlinked Passenger Trips FY per Capita by Organization Category (Outliers Removed > 95%)')
plt.xlabel('Trips per Capita')
plt.ylabel('Organization Category')
# Log scale might still be useful, but check if it's necessary after removing extreme outliers
# plt.xscale('log')
plt.tight_layout()
plt.show()

# Violin plot with outliers removed
plt.figure(figsize=(12, 7))
sb.violinplot(x='Trips per Capita', y='Org Category', data=organization_analysis_df_no_outliers, palette='viridis', inner='quartile')
plt.title('Distribution (Density) of Unlinked Passenger Trips FY per Capita by Organization Category (Outliers Removed > 95%)')
plt.xlabel('Trips per Capita')
plt.ylabel('Organization Category')
# Log scale might still be useful, but check if it's necessary after removing extreme outliers
# plt.xscale('log')
plt.tight_layout()
plt.show()

# prompt: Can you run the test again for 80% outlier detection

# Apply the outlier removal function to the 'Trips per Capita' column
# Let's remove outliers above the 80th percentile in the 'Trips per Capita' column
organization_analysis_df_no_outliers_80 = remove_outliers_above_percentile(
  organization_analysis_df, 'Trips per Capita', percentile=80
)

print("\nDataFrame after removing outliers in 'Trips per Capita' (80th percentile):")
print(organization_analysis_df_no_outliers_80[['Organization Type', 'Org Category', 'Trips per Capita']].head())


# Now, repeat the analysis using the DataFrame with outliers removed (80th percentile)

# Group by 'Org Category' and calculate the average normalized trips
category_avg_normalized_trips_no_outliers_80 = organization_analysis_df_no_outliers_80.groupby('Org Category')['Trips per Capita'].mean().reset_index()

# Sort the results by average normalized trips in descending order
category_avg_normalized_trips_no_outliers_80_sorted = category_avg_normalized_trips_no_outliers_80.sort_values(by='Trips per Capita', ascending=False)

print("\nAverage Trips per Capita by Org Category (Outliers Removed > 80%):")
print(category_avg_normalized_trips_no_outliers_80_sorted)

# Create a bar plot of average normalized trips by organization category (with outliers removed > 80%)
plt.figure(figsize=(10, 6))
sb.barplot(x='Trips per Capita', y='Org Category', data=category_avg_normalized_trips_no_outliers_80_sorted, palette='viridis')
plt.title('Average Unlinked Passenger Trips FY per Capita by Organization Category (Outliers Removed > 80%)')
plt.xlabel('Average Trips per Capita')
plt.ylabel('Organization Category')
plt.tight_layout()
plt.show()

# Prepare data for Mann-Whitney U test (80th percentile outlier removal)
public_trips_per_capita_80 = organization_analysis_df_no_outliers_80[
    organization_analysis_df_no_outliers_80['Org Category'] == 'Public'
]['Trips per Capita'].dropna() # Ensure we don't have NaNs

private_trips_per_capita_80 = organization_analysis_df_no_outliers_80[
    organization_analysis_df_no_outliers_80['Org Category'] == 'Private'
]['Trips per Capita'].dropna() # Ensure we don't have NaNs

print(f"\nNumber of data points for Public category (80% outlier removal): {len(public_trips_per_capita_80)}")
print(f"Number of data points for Private category (80% outlier removal): {len(private_trips_per_capita_80)}")

# Check if there are enough data points in each group for the test
if len(public_trips_per_capita_80) > 0 and len(private_trips_per_capita_80) > 0:
    # Perform the Mann-Whitney U test
    try:
        u_statistic_80, p_value_80 = mannwhitneyu(public_trips_per_capita_80, private_trips_per_capita_80, alternative='two-sided')

        print(f"\nMann-Whitney U Test Results (Outliers Removed > 80%):")
        print(f"U Statistic: {u_statistic_80:.4f}")
        print(f"P-value: {p_value_80:.4f}")

        # Interpret the results
        alpha = 0.05  # Significance level
        if p_value_80 < alpha:
            print(f"The p-value ({p_value_80:.4f}) is less than the significance level ({alpha}).")
            print("We reject the null hypothesis.")
            print("There is a statistically significant difference in the distribution of 'Trips per Capita' between Public and Private organizations (Outliers Removed > 80%).")
        else:
            print(f"The p-value ({p_value_80:.4f}) is greater than or equal to the significance level ({alpha}).")
            print("We fail to reject the null hypothesis.")
            print("There is no statistically significant evidence to suggest a difference in the distribution of 'Trips per Capita' between Public and Private organizations (Outliers Removed > 80%).")

    except ValueError as e:
        print(f"\nError performing Mann-Whitney U test on 80% outlier removed data: {e}")
        print("This might happen if one of the groups is empty or has very few data points.")

else:
    print("\nCannot perform Mann-Whitney U test on 80% outlier removed data. One or both groups (Public/Private) have no or insufficient data after 80% outlier removal.")


# Plot the distribution using the data *after* removing outliers (> 80%)
print("\n--- Plotting Distribution After Outlier Removal (> 80%) ---")

# Box plot with outliers removed (> 80%)
plt.figure(figsize=(12, 7))
sb.boxplot(x='Trips per Capita', y='Org Category', data=organization_analysis_df_no_outliers_80, palette='viridis')
plt.title('Distribution of Unlinked Passenger Trips FY per Capita by Organization Category (Outliers Removed > 80%)')
plt.xlabel('Trips per Capita')
plt.ylabel('Organization Category')
# Log scale might still be useful, but check if it's necessary after removing extreme outliers
# plt.xscale('log')
plt.tight_layout()
plt.show()

# Violin plot with outliers removed (> 80%)
plt.figure(figsize=(12, 7))
sb.violinplot(x='Trips per Capita', y='Org Category', data=organization_analysis_df_no_outliers_80, palette='viridis', inner='quartile')
plt.title('Distribution (Density) of Unlinked Passenger Trips FY per Capita by Organization Category (Outliers Removed > 80%)')
plt.xlabel('Trips per Capita')
plt.ylabel('Organization Category')
# Log scale might still be useful, but check if it's necessary after removing extreme outliers
# plt.xscale('log')
plt.tight_layout()
plt.show()

# prompt: Can you add a comparison between Avg Fares Per Trip FY? There is no need to normalize these values

# Select the relevant columns for Fare analysis
fare_analysis_df = df[['NTD ID', 'Organization Type', 'Avg Fares Per Trip FY']].copy()

# Clean 'Avg Fares Per Trip FY' column (assuming it needs similar cleaning as other numeric columns)
# Reusing the previously defined safe_numeric_conversion function
fare_analysis_df['Avg Fares Per Trip FY_numeric'] = fare_analysis_df['Avg Fares Per Trip FY'].apply(safe_numeric_conversion)

# Drop rows with NaN in the numeric fare column
fare_analysis_df.dropna(subset=['Avg Fares Per Trip FY_numeric'], inplace=True)

print("\nDataFrame with 'Avg Fares Per Trip FY' added and cleaned:")
print(fare_analysis_df[['NTD ID', 'Organization Type', 'Avg Fares Per Trip FY_numeric']].head())

# Group by 'Organization Type' and calculate the average of 'Avg Fares Per Trip FY_numeric'
organization_avg_fares = fare_analysis_df.groupby('Organization Type')['Avg Fares Per Trip FY_numeric'].mean().reset_index()

# Sort the results by average fare in descending order
organization_avg_fares_sorted = organization_avg_fares.sort_values(by='Avg Fares Per Trip FY_numeric', ascending=False)

print("\nAverage Fares Per Trip FY by Organization Type:")
print(organization_avg_fares_sorted)

# Create a bar plot of average fares per trip by organization type
plt.figure(figsize=(12, 7))
sb.barplot(x='Avg Fares Per Trip FY_numeric', y='Organization Type', data=organization_avg_fares_sorted, palette='viridis')
plt.title('Average Fares Per Trip FY by Organization Type')
plt.xlabel('Average Fares Per Trip FY')
plt.ylabel('Organization Type')
plt.tight_layout()
plt.show()


# Create a new column 'Org Category' for the fare analysis DataFrame
def categorize_organization(org_type):
    if org_type in public_types:
        return 'Public'
    elif org_type in private_types:
        return 'Private'
    else:
        return 'Other' # For any types not explicitly listed

fare_analysis_df['Org Category'] = fare_analysis_df['Organization Type'].apply(categorize_organization)

print("\nDataFrame with 'Org Category' for Fare Analysis:")
print(fare_analysis_df[['Organization Type', 'Org Category', 'Avg Fares Per Trip FY_numeric']].head())

# Group by 'Org Category' and calculate the average of 'Avg Fares Per Trip FY_numeric'
category_avg_fares = fare_analysis_df.groupby('Org Category')['Avg Fares Per Trip FY_numeric'].mean().reset_index()

# Sort the results by average fare in descending order
category_avg_fares_sorted = category_avg_fares.sort_values(by='Avg Fares Per Trip FY_numeric', ascending=False)

print("\nAverage Fares Per Trip FY by Org Category:")
print(category_avg_fares_sorted)

# Create a bar plot of average fares per trip by organization category
plt.figure(figsize=(10, 6))
sb.barplot(x='Avg Fares Per Trip FY_numeric', y='Org Category', data=category_avg_fares_sorted, palette='viridis')
plt.title('Average Fares Per Trip FY by Organization Category')
plt.xlabel('Average Fares Per Trip FY')
plt.ylabel('Organization Category')
plt.tight_layout()
plt.show()

# Perform Mann-Whitney U test on Avg Fares Per Trip FY by Org Category

# Prepare data for Mann-Whitney U test
public_fares = fare_analysis_df[
    fare_analysis_df['Org Category'] == 'Public'
]['Avg Fares Per Trip FY_numeric'].dropna() # Ensure no NaNs

private_fares = fare_analysis_df[
    fare_analysis_df['Org Category'] == 'Private'
]['Avg Fares Per Trip FY_numeric'].dropna() # Ensure no NaNs

print(f"\nNumber of data points for Public category (Avg Fares): {len(public_fares)}")
print(f"Number of data points for Private category (Avg Fares): {len(private_fares)}")

# Check if there are enough data points in each group for the test
if len(public_fares) > 0 and len(private_fares) > 0:
    # Perform the Mann-Whitney U test
    # H0: The distributions of Avg Fares Per Trip FY for Public and Private organizations are the same.
    # H1: The distributions are different.
    try:
        u_statistic_fares, p_value_fares = mannwhitneyu(public_fares, private_fares, alternative='two-sided')

        print(f"\nMann-Whitney U Test Results (Avg Fares Per Trip FY):")
        print(f"U Statistic: {u_statistic_fares:.4f}")
        print(f"P-value: {p_value_fares:.4f}")

        # Interpret the results
        alpha = 0.05  # Significance level
        if p_value_fares < alpha:
            print(f"The p-value ({p_value_fares:.4f}) is less than the significance level ({alpha}).")
            print("We reject the null hypothesis.")
            print("There is a statistically significant difference in the distribution of 'Avg Fares Per Trip FY' between Public and Private organizations.")
        else:
            print(f"The p-value ({p_value_fares:.4f}) is greater than or equal to the significance level ({alpha}).")
            print("We fail to reject the null hypothesis.")
            print("There is no statistically significant evidence to suggest a difference in the distribution of 'Avg Fares Per Trip FY' between Public and Private organizations.")

    except ValueError as e:
        print(f"\nError performing Mann-Whitney U test on Avg Fares: {e}")
        print("This might happen if one of the groups is empty or has very few data points.")

else:
    print("\nCannot perform Mann-Whitney U test on Avg Fares. One or both groups (Public/Private) have no or insufficient data.")


# Plot the distribution of Avg Fares Per Trip FY by Org Category
print("\n--- Plotting Distribution of Avg Fares Per Trip FY ---")

# Box plot
plt.figure(figsize=(12, 7))
sb.boxplot(x='Avg Fares Per Trip FY_numeric', y='Org Category', data=fare_analysis_df, palette='viridis')
plt.title('Distribution of Average Fares Per Trip FY by Organization Category')
plt.xlabel('Average Fares Per Trip FY')
plt.ylabel('Organization Category')
# Consider using log scale if the data is heavily skewed, or remove outliers first
# plt.xscale('log')
plt.tight_layout()
plt.show()

# Violin plot
plt.figure(figsize=(12, 7))
sb.violinplot(x='Avg Fares Per Trip FY_numeric', y='Org Category', data=fare_analysis_df, palette='viridis', inner='quartile')
plt.title('Distribution (Density) of Average Fares Per Trip FY by Organization Category')
plt.xlabel('Average Fares Per Trip FY')
plt.ylabel('Organization Category')
# Consider using log scale if the data is heavily skewed, or remove outliers first
# plt.xscale('log')
plt.tight_layout()
plt.show()

# Optional: Remove outliers in Avg Fares and re-plot/test
fare_analysis_df_no_outliers_95 = remove_outliers_above_percentile(
    fare_analysis_df, 'Avg Fares Per Trip FY_numeric', percentile=95
)

print("\n--- Analysis After Outlier Removal (> 95%) for Avg Fares ---")

# Group by 'Org Category' and calculate the average of 'Avg Fares Per Trip FY_numeric'
category_avg_fares_no_outliers_95 = fare_analysis_df_no_outliers_95.groupby('Org Category')['Avg Fares Per Trip FY_numeric'].mean().reset_index()
category_avg_fares_no_outliers_95_sorted = category_avg_fares_no_outliers_95.sort_values(by='Avg Fares Per Trip FY_numeric', ascending=False)

print("\nAverage Fares Per Trip FY by Org Category (Outliers Removed > 95%):")
print(category_avg_fares_no_outliers_95_sorted)

# Create a bar plot (with outliers removed > 95%)
plt.figure(figsize=(10, 6))
sb.barplot(x='Avg Fares Per Trip FY_numeric', y='Org Category', data=category_avg_fares_no_outliers_95_sorted, palette='viridis')
plt.title('Average Fares Per Trip FY by Org Category (Outliers Removed > 95%)')
plt.xlabel('Average Fares Per Trip FY')
plt.ylabel('Organization Category')
plt.tight_layout()
plt.show()


# Mann-Whitney U test (with outliers removed > 95%)
public_fares_95 = fare_analysis_df_no_outliers_95[
    fare_analysis_df_no_outliers_95['Org Category'] == 'Public'
]['Avg Fares Per Trip FY_numeric'].dropna()

private_fares_95 = fare_analysis_df_no_outliers_95[
    fare_analysis_df_no_outliers_95['Org Category'] == 'Private'
]['Avg Fares Per Trip FY_numeric'].dropna()

print(f"\nNumber of data points for Public category (Avg Fares, >95% outlier removal): {len(public_fares_95)}")
print(f"Number of data points for Private category (Avg Fares, >95% outlier removal): {len(private_fares_95)}")

if len(public_fares_95) > 0 and len(private_fares_95) > 0:
    try:
        u_statistic_fares_95, p_value_fares_95 = mannwhitneyu(public_fares_95, private_fares_95, alternative='two-sided')
        print(f"\nMann-Whitney U Test Results (Avg Fares Per Trip FY, Outliers Removed > 95%):")
        print(f"U Statistic: {u_statistic_fares_95:.4f}")
        print(f"P-value: {p_value_fares_95:.4f}")
        alpha = 0.05
        if p_value_fares_95 < alpha:
            print("Statistically significant difference.")
        else:
            print("No statistically significant difference.")
    except ValueError as e:
         print(f"\nError performing Mann-Whitney U test on Avg Fares (>95% outlier removed): {e}")
else:
     print("\nCannot perform Mann-Whitney U test on Avg Fares (>95% outlier removed). Insufficient data.")

# Plot distributions (with outliers removed > 95%)
plt.figure(figsize=(12, 7))
sb.boxplot(x='Avg Fares Per Trip FY_numeric', y='Org Category', data=fare_analysis_df_no_outliers_95, palette='viridis')
plt.title('Distribution of Average Fares Per Trip FY by Organization Category (Outliers Removed > 95%)')
plt.xlabel('Average Fares Per Trip FY')
plt.ylabel('Organization Category')
plt.tight_layout()
plt.show()

plt.figure(figsize=(12, 7))
sb.violinplot(x='Avg Fares Per Trip FY_numeric', y='Org Category', data=fare_analysis_df_no_outliers_95, palette='viridis', inner='quartile')
plt.title('Distribution (Density) of Average Fares Per Trip FY by Organization Category (Outliers Removed > 95%)')
plt.xlabel('Average Fares Per Trip FY')
plt.ylabel('Organization Category')
plt.tight_layout()
plt.show()

# Check numeric columns in the DataFrame
df.select_dtypes(include=['number']).columns.tolist()

ALPHA = 0.05  # significance level

# --- Helpers: quick checks and summaries ---
def list_numeric_columns(df):
    """Return a list of numeric columns for quick reference."""
    return df.select_dtypes(include=[np.number]).columns.tolist()

def safe_group_vectors(df, y_col, group_col):
    """
    Prepare grouped numeric vectors for tests.
    Returns: list of np.arrays (one per group), and a list of group labels.
    """
    df_sub = df[[y_col, group_col]].dropna()
    groups = []
    labels = []
    for lbl, g in df_sub.groupby(group_col):
        vec = g[y_col].to_numpy()
        if len(vec) > 0:
            groups.append(vec)
            labels.append(lbl)
    return groups, labels

# --- Test 1: ANOVA or Kruskal (auto-selected based on assumptions) ---
def run_anova_or_kruskal(df, y_col, group_col, alpha=ALPHA):
    # Validate columns
    for c in (y_col, group_col):
        if c not in df.columns:
            raise KeyError(f"Column not found: {c}")

    # Build groups
    groups, labels = safe_group_vectors(df, y_col, group_col)
    if len(groups) < 2:
        print(f"⚠️ Not enough groups in '{group_col}' (found {len(groups)}).")
        return None

    # Normality (Shapiro–Wilk) for groups with n>=3
    normal_flags = []
    for vec in groups:
        if len(vec) >= 3:
            p_shapiro = stats.shapiro(vec)[1]
            normal_flags.append(p_shapiro > alpha)
        else:
            # For very small groups, skip normality requirement
            normal_flags.append(False)
    normal_groups = all(normal_flags) and len(normal_flags) > 0

    # Homogeneity of variances (Levene’s test) if possible
    try:
        p_levene = stats.levene(*groups)[1]
        equal_var = p_levene > alpha
    except Exception:
        equal_var = False

    # Choose test
    if normal_groups and equal_var and len(groups) >= 2:
        test_name = "ANOVA (parametric)"
        stat, p = stats.f_oneway(*groups)
    else:
        test_name = "Kruskal–Wallis (non-parametric)"
        stat, p = stats.kruskal(*groups)

    # Report
    print(f"{test_name} — {group_col} vs {y_col}")
    print(f"stat = {stat:.4f}, p = {p:.6f}, alpha = {alpha}")
    if p < alpha:
        print("✅ Significant difference between groups.")
    else:
        print("❌ No significant difference between groups.")

    # Optional: return a compact result dict
    return {
        "test": test_name,
        "stat": float(stat),
        "p_value": float(p),
        "alpha": float(alpha),
        "k_groups": len(groups),
        "groups": labels,
    }

# Example run (update names if needed):
# Dependent numeric: "Avg Trip Length FY"
# Grouping categorical: "3 Mode"
anova_result = run_anova_or_kruskal(df, "Avg Trip Length FY", "3 Mode")

# --- Test 2: Chi-square for association between two categorical variables ---
def run_chi_square(df, col_a, col_b, alpha=ALPHA):
    # Validate columns
    for c in (col_a, col_b):
        if c not in df.columns:
            raise KeyError(f"Column not found: {c}")

    # Build contingency table
    tab = pd.crosstab(df[col_a], df[col_b])
    if tab.empty or tab.shape[0] < 2 or tab.shape[1] < 2:
        print(f"⚠️ Contingency table too small for Chi-square: shape={tab.shape}")
        return None, None

    chi2, p, dof, expected = stats.chi2_contingency(tab)

    # Report
    print(f"Chi-square — {col_a} × {col_b}")
    print(f"chi2 = {chi2:.4f}, p = {p:.6f}, dof = {dof}, alpha = {alpha}")
    if p < alpha:
        print("✅ Variables are associated (reject independence).")
    else:
        print("❌ No evidence of association (fail to reject independence).")

    return tab, pd.DataFrame(expected, index=tab.index, columns=tab.columns)

# Example run:
# Categorical vs categorical: "3 Mode" × "Org Category"
chi_tab, chi_exp = run_chi_square(df, "3 Mode", "Organization Type")
display(chi_tab)


# Optional: display the contingency table
if chi_tab is not None:
    display(chi_tab)

# --- Quick check of numeric columns, if helpful ---
print("Numeric columns:", list_numeric_columns(df))

from scipy import stats

def run_f_test_anova(df, y_col, group_col, alpha=0.05):
    # Prepare groups
    groups = [g[y_col].dropna().values for _, g in df.groupby(group_col)]

    # Run ANOVA
    f_stat, p = stats.f_oneway(*groups)

    print(f"F-test (ANOVA) — {group_col} vs {y_col}")
    print(f"F = {f_stat:.4f}, p = {p:.6f}, alpha = {alpha}")

    if p < alpha:
        print("✅ Significant difference between group means.")
    else:
        print("❌ No significant difference between group means.")

    return f_stat, p

# Example: Avg Trip Length FY across 3 Mode
run_f_test_anova(df, "Avg Trip Length FY", "3 Mode")

def check_normality(df, y_col, group_col, alpha=0.05):
    """Run Shapiro–Wilk normality test for each group."""
    results = []
    for name, group in df.groupby(group_col):
        data = group[y_col].dropna()
        if len(data) >= 3:
            stat, p = stats.shapiro(data)
            results.append({"group": name, "n": len(data), "stat": stat, "p": p})
    res_df = pd.DataFrame(results)
    display(res_df)
    print(f"Interpretation: p < {alpha} means the group is NOT normal.")
    return res_df

# Example
check_normality(df, "Avg Trip Length FY", "3 Mode")

# Effect size for categorical association (Cramér's V), plus bias-corrected version.
import numpy as np, pandas as pd
from scipy import stats

def cramers_v_table(df, a="3 Mode", b="Organization Type"):
    tab = pd.crosstab(df[a], df[b])
    chi2, p, dof, exp = stats.chi2_contingency(tab)
    n = tab.values.sum()
    r, c = tab.shape
    # Standard Cramér's V
    V = np.sqrt(chi2 / (n * (min(r-1, c-1))))
    # Bias-corrected V (Bergsma & Wicher, 2013)
    phi2 = chi2 / n
    phi2_corr = max(0, phi2 - ((c-1)*(r-1)) / (n-1))
    r_corr = r - ((r-1)**2) / (n-1)
    c_corr = c - ((c-1)**2) / (n-1)
    denom = min(r_corr-1, c_corr-1)
    V_corr = np.sqrt(phi2_corr / denom) if denom > 0 else np.nan
    print(f"Chi2={chi2:.4f}, p={p:.6g}, dof={dof}, n={n}")
    print(f"Cramér's V={V:.3f} | bias-corrected V={V_corr:.3f}")
    return tab, V, V_corr

# Example:
tab, V, Vc = cramers_v_table(df, "3 Mode", "Organization Type")

# Which cells contribute most? (standardized residuals)
def chi_top_residuals(df, a="3 Mode", b="Organization Type", top_k=10):
    tab = pd.crosstab(df[a], df[b])
    chi2, p, dof, exp = stats.chi2_contingency(tab)
    resid = (tab - exp) / np.sqrt(exp)
    offenders = (
        resid.stack()
             .rename("std_resid")
             .reset_index()
             .assign(abs_res=lambda x: x["std_resid"].abs())
             .sort_values("abs_res", ascending=False)
             .head(top_k)
    )
    print(f"Chi2={chi2:.4f}, p={p:.6g}, dof={dof}")
    display(tab)
    print(f"\nTop {top_k} cells by |standardized residual|:")
    display(offenders)
    return resid, offenders

# Example:
resid, top = chi_top_residuals(df, "3 Mode", "Organization Type", top_k=12)

# Pairwise chi-square across mode levels with BH-FDR correction.
import numpy as np
from itertools import combinations
from scipy import stats

def pairwise_chi_by_mode(df, mode_col="3 Mode", org_col="Organization Type", alpha=0.05):
    levels = [x for x in df[mode_col].dropna().unique()]
    rows = []
    for a, b in combinations(levels, 2):
        sub = df[df[mode_col].isin([a, b])]
        tab = pd.crosstab(sub[mode_col], sub[org_col])
        if tab.shape[0] == 2 and tab.shape[1] >= 2:
            chi2, p, dof, exp = stats.chi2_contingency(tab)
            rows.append({"pair": f"{a} vs {b}", "chi2": chi2, "p": p, "dof": dof, "n": int(tab.values.sum())})
    res = pd.DataFrame(rows).sort_values("p")
    if not res.empty:
        p = res["p"].values
        order = np.argsort(p); ranked = p[order]
        n = len(p); adj = np.empty(n); cummin = 1.0
        for i in range(n-1, -1, -1):
            val = ranked[i]*n/(i+1)
            cummin = min(cummin, val); adj[i] = min(cummin, 1.0)
        res["p_adj"] = adj[np.argsort(order)]
        res["sig"] = res["p_adj"] < alpha
    display(res)
    return res

# Example:
pairwise = pairwise_chi_by_mode(df, "3 Mode", "Organization Type", alpha=0.05)

# Cell 1 — Chi-square diagnostics: contributions (% of chi2) + residuals heatmap (optional)
import numpy as np, pandas as pd
from scipy import stats
import matplotlib.pyplot as plt

def chi2_diagnostics(df, a="3 Mode", b="Organization Type", top_k=12, plot_heatmap=True):
    tab = pd.crosstab(df[a], df[b])
    chi2, p, dof, exp = stats.chi2_contingency(tab)
    contrib = (tab - exp)**2 / exp
    resid = (tab - exp) / np.sqrt(exp)

    total = contrib.values.sum()
    flat = (
        contrib.stack()
               .rename("chi2_contrib")
               .reset_index()
               .assign(perc=lambda x: 100*x["chi2_contrib"]/total)
               .sort_values("chi2_contrib", ascending=False)
               .head(top_k)
    )
    print(f"Chi2={chi2:.4f}, p={p:.3g}, dof={dof}, n={int(tab.values.sum())}")
    print(f"Top {top_k} cells by chi-square contribution (% of total):")
    display(flat)

    # Aggregate contributions per row/col
    by_row = contrib.sum(axis=1).to_frame("chi2_row_contrib").assign(perc=lambda x:100*x["chi2_row_contrib"]/total)
    by_col = contrib.sum(axis=0).to_frame("chi2_col_contrib").assign(perc=lambda x:100*x["chi2_col_contrib"]/total)
    print("\nRow contributions (% of total):"); display(by_row.sort_values("chi2_row_contrib", ascending=False))
    print("\nColumn contributions (% of total):"); display(by_col.sort_values("chi2_col_contrib", ascending=False))

    if plot_heatmap:
        plt.figure()
        plt.imshow(resid.values, aspect="auto")
        plt.title("Standardized residuals")
        plt.xticks(ticks=range(tab.shape[1]), labels=tab.columns, rotation=45, ha="right")
        plt.yticks(ticks=range(tab.shape[0]), labels=tab.index)
        plt.colorbar()
        plt.tight_layout()
        plt.show()

    return {"tab": tab, "expected": pd.DataFrame(exp, index=tab.index, columns=tab.columns),
            "residuals": resid, "contrib": contrib, "top": flat}

# Example:
diag = chi2_diagnostics(df, "3 Mode", "Organization Type", top_k=12, plot_heatmap=True)

# Cell 2 — Robustness: collapse rare Organization Type levels -> rerun chi-square + Cramér's V
def collapse_rare(df, col, min_count=20, other_name="Other"):
    vc = df[col].value_counts()
    keep = vc[vc >= min_count].index
    out = df.copy()
    out[col] = np.where(out[col].isin(keep), out[col], other_name)
    return out

def chi2_with_cramers_v(df, a="3 Mode", b="Organization Type"):
    tab = pd.crosstab(df[a], df[b])
    chi2, p, dof, exp = stats.chi2_contingency(tab)
    n = tab.values.sum()
    r, c = tab.shape
    V = np.sqrt(chi2 / (n * (min(r-1, c-1))))
    print(f"Chi2={chi2:.4f}, p={p:.3g}, dof={dof}, n={n}, Cramér's V={V:.3f}")
    return tab, V

# Example:
df_collapsed = collapse_rare(df, "Organization Type", min_count=20, other_name="Other")
tab_c, V_c = chi2_with_cramers_v(df_collapsed, "3 Mode", "Organization Type")

df.columns.tolist()

# --- Patch: coerce metric columns to numeric inside the function ---

import numpy as np
import pandas as pd
from scipy import stats

def _to_num(s):
    s = (s.astype(str)
          .str.replace(r'[,\s]', '', regex=True)        # remove commas/spaces
          .str.replace(r'[$€₪%]', '', regex=True)       # drop currency/percent
          .str.replace('−', '-', regex=False)           # minus sign variant
          .str.replace(r'\(([^)]+)\)', r'-\1', regex=True))  # (123) -> -123
    return pd.to_numeric(s, errors='coerce')

def compare_pp_within_modes(
    df,
    metrics=("Trips per Capita","Avg Fares Per Trip FY","Avg Trip Length FY"),
    mode_col_candidates=("3 Mode","Mode"),
    alpha=0.05
):
    # pick mode column
    mode_col = next((c for c in mode_col_candidates if c in df.columns), None)
    if mode_col is None:
        raise KeyError(f"No mode column found among: {mode_col_candidates}")

    # ensure Public/Private column
    ORG_MAP = {
        "Private-For-Profit Corporation": "Private",
        "Private-Non-Profit Corporation": "Private",
        "Private Provider Reporting on Behalf of a Public Entity": "Private",
        "City, County or Local Government Unit or Department of Transportation": "Public",
        "Independent Public Agency or Authority of Transit Service": "Public",
        "MPO, COG or Other Planning Agency": "Public",
        "Other Publicly-Owned or Privately Chartered Corporation": "Public",
        "State Government Unit or Department of Transportation": "Public",
        "Subsidiary Unit of a Transit Agency, Reporting Separately": "Public",
        "University": "Public",
    }
    if "Org Category" in df.columns:
        cat_col = "Org Category"
        work = df.copy()
    elif "Organization Type" in df.columns:
        cat_col = "Org Category (derived)"
        work = df.copy()
        work[cat_col] = work["Organization Type"].map(ORG_MAP)
    else:
        raise KeyError("Need 'Org Category' or 'Organization Type'.")

    work = work.dropna(subset=[cat_col])
    work = work[work[cat_col].isin(["Public","Private"])]

    # keep & coerce metrics
    metrics = [m for m in metrics if m in work.columns]
    if not metrics:
        raise KeyError("None of the requested metrics exist in the DataFrame.")
    for m in metrics:
        work[m] = _to_num(work[m])  # <-- numeric coercion here

    rows_t, rows_u = [], []
    for m in metrics:
        for mode_val, g in work.groupby(mode_col):
            a = g.loc[g[cat_col]=="Public", m].dropna().to_numpy()
            b = g.loc[g[cat_col]=="Private", m].dropna().to_numpy()
            n1, n2 = len(a), len(b)
            if n1 < 2 or n2 < 2:
                continue

            # Welch t-test
            t_stat, p_t = stats.ttest_ind(a, b, equal_var=False)

            # Mann–Whitney U
            u_stat, p_u = stats.mannwhitneyu(a, b, alternative="two-sided")

            rows_t.append({"metric": m, "mode": mode_val, "test": "Welch t",
                           "n_public": n1, "n_private": n2,
                           "mean_public": float(np.mean(a)), "mean_private": float(np.mean(b)),
                           "t_stat": float(t_stat), "p": float(p_t)})
            rows_u.append({"metric": m, "mode": mode_val, "test": "Mann–Whitney U",
                           "n_public": n1, "n_private": n2,
                           "median_public": float(np.median(a)), "median_private": float(np.median(b)),
                           "u_stat": float(u_stat), "p": float(p_u)})

    t_df = pd.DataFrame(rows_t).sort_values(["metric","mode"]).reset_index(drop=True)
    u_df = pd.DataFrame(rows_u).sort_values(["metric","mode"]).reset_index(drop=True)

    print(f"Used mode column: {mode_col} | group column: {cat_col}")
    if not t_df.empty:
        print("\nWelch t-tests (Public vs Private within each Mode):")
        display(t_df)
    if not u_df.empty:
        print("\nMann–Whitney U (Public vs Private within each Mode):")
        display(u_df)

    return t_df, u_df

t_results, u_results = compare_pp_within_modes(
    df,
    metrics=("Trips per Capita","Avg Fares Per Trip FY","Avg Trip Length FY"),
    mode_col_candidates=("3 Mode","Mode"),
    alpha=0.05
)

# Summarize Public vs Private within each Mode:
# - BH-FDR correction
# - Direction (Private>Public or Public>Private)
# - Min sample size filter
# - Consensus (significant in BOTH tests)

import numpy as np
import pandas as pd

def bh_fdr(pvals, alpha=0.05):
    p = np.asarray(pvals, dtype=float)
    n = len(p)
    order = np.argsort(p)
    ranked = p[order]
    adj = np.empty(n, float); cm = 1.0
    for i in range(n-1, -1, -1):
        val = ranked[i] * n / (i+1)
        cm = min(cm, val)
        adj[i] = min(cm, 1.0)
    out = np.empty(n, float); out[order] = adj
    return out

def summarize_pp_results(t_df, u_df, alpha=0.05, min_n=5):
    t = t_df.copy()
    u = u_df.copy()

    # filter tiny groups
    t = t[(t["n_public"]>=min_n) & (t["n_private"]>=min_n)]
    u = u[(u["n_public"]>=min_n) & (u["n_private"]>=min_n)]

    # FDR
    if not t.empty:
        t["p_adj"] = bh_fdr(t["p"].values, alpha=alpha)
        t["sig"] = t["p_adj"] < alpha
        t["direction"] = np.where(t["mean_private"]>t["mean_public"], "Private>Public", "Public>Private")
    if not u.empty:
        u["p_adj"] = bh_fdr(u["p"].values, alpha=alpha)
        u["sig"] = u["p_adj"] < alpha
        u["direction"] = np.where(u["median_private"]>u["median_public"], "Private>Public", "Public>Private")

    # display significant only
    t_sig = t[t["sig"]].sort_values(["metric","mode","p_adj"])
    u_sig = u[u["sig"]].sort_values(["metric","mode","p_adj"])

    print("=== Welch t-tests (BH-FDR, significant only) ===")
    if not t_sig.empty:
        display(t_sig[["metric","mode","n_public","n_private","mean_public","mean_private","direction","p_adj"]])
    else:
        print("No significant results after FDR.")

    print("\n=== Mann–Whitney U (BH-FDR, significant only) ===")
    if not u_sig.empty:
        display(u_sig[["metric","mode","n_public","n_private","median_public","median_private","direction","p_adj"]])
    else:
        print("No significant results after FDR.")

    # consensus: significant in BOTH tests
    consensus = (
        t_sig.merge(u_sig, on=["metric","mode"], how="inner", suffixes=("_t","_u"))
             .sort_values(["metric","mode"])
    )
    print("\n=== CONSENSUS (significant in BOTH tests) ===")
    if not consensus.empty:
        display(consensus[["metric","mode","direction_t","direction_u","p_adj_t","p_adj_u",
                           "n_public_t","n_private_t"]])
    else:
        print("No overlaps between tests (after FDR).")

    return t, u, consensus

# Run the summary on your results
t_fdr, u_fdr, consensus = summarize_pp_results(t_results, u_results, alpha=0.05, min_n=5)

# === Effect sizes + bootstrap CIs for Private − Public within each Mode (per metric) ===
import numpy as np, pandas as pd
import matplotlib.pyplot as plt
from scipy import stats

# Map Organization Type -> Public/Private (used if "Org Category" is missing)
ORG_MAP = {
    "Private-For-Profit Corporation": "Private",
    "Private-Non-Profit Corporation": "Private",
    "Private Provider Reporting on Behalf of a Public Entity": "Private",
    "City, County or Local Government Unit or Department of Transportation": "Public",
    "Independent Public Agency or Authority of Transit Service": "Public",
    "MPO, COG or Other Planning Agency": "Public",
    "Other Publicly-Owned or Privately Chartered Corporation": "Public",
    "State Government Unit or Department of Transportation": "Public",
    "Subsidiary Unit of a Transit Agency, Reporting Separately": "Public",
    "University": "Public",
}

def _to_num(s):
    s = (s.astype(str)
          .str.replace(r'[,\s]', '', regex=True)
          .str.replace(r'[$€₪%]', '', regex=True)
          .str.replace('−', '-', regex=False)
          .str.replace(r'\(([^)]+)\)', r'-\1', regex=True))
    return pd.to_numeric(s, errors='coerce')

def _groups(df, mode_col, mode_val, cat_col, metric):
    g = df[df[mode_col]==mode_val]
    a = g.loc[g[cat_col]=="Public", metric].dropna().to_numpy()
    b = g.loc[g[cat_col]=="Private", metric].dropna().to_numpy()
    return a, b

def hedges_g(a, b):
    n1, n2 = len(a), len(b)
    if n1<2 or n2<2: return np.nan
    s1, s2 = np.var(a, ddof=1), np.var(b, ddof=1)
    sp = np.sqrt(((n1-1)*s1 + (n2-1)*s2) / (n1+n2-2)) if (n1+n2-2)>0 else np.nan
    if not np.isfinite(sp) or sp==0: return np.nan
    d = (np.mean(b) - np.mean(a)) / sp   # Private − Public
    J = 1 - (3/(4*(n1+n2)-9)) if (n1+n2)>2 else 1.0
    return d*J

def cliffs_delta_from_u(u, n1, n2):
    return 2.0*u/(n1*n2) - 1.0   # Private − Public

def bootstrap_diff(a, b, fn=np.mean, n_boot=4000, rng=np.random.default_rng(0)):
    """Bootstrap CI for (fn(b) − fn(a))."""
    if len(a)==0 or len(b)==0: return np.nan, (np.nan, np.nan)
    diffs = []
    for _ in range(n_boot):
        sa = rng.choice(a, size=len(a), replace=True)
        sb = rng.choice(b, size=len(b), replace=True)
        diffs.append(fn(sb) - fn(sa))
    diffs = np.asarray(diffs)
    return float(np.mean(diffs)), (float(np.percentile(diffs,2.5)), float(np.percentile(diffs,97.5)))

def effect_sizes_and_CIs(df,
                         metrics=("Avg Fares Per Trip FY","Avg Trip Length FY"),
                         mode_col_candidates=("3 Mode","Mode"),
                         alpha=0.05, min_n=5, n_boot=4000):
    # choose mode col
    mode_col = next((c for c in mode_col_candidates if c in df.columns), None)
    if mode_col is None: raise KeyError(f"No mode column among: {mode_col_candidates}")

    # ensure Public/Private column
    if "Org Category" in df.columns:
        cat_col = "Org Category"
        work = df.copy()
    elif "Organization Type" in df.columns:
        cat_col = "Org Category (derived)"
        work = df.copy(); work[cat_col] = work["Organization Type"].map(ORG_MAP)
    else:
        raise KeyError("Need 'Org Category' or 'Organization Type'.")

    work = work.dropna(subset=[cat_col, mode_col])
    work = work[work[cat_col].isin(["Public","Private"])]

    # coerce metrics to numeric
    use_metrics = [m for m in metrics if m in work.columns]
    if not use_metrics: raise KeyError("None of the requested metrics exist.")
    for m in use_metrics: work[m] = _to_num(work[m])

    rows = []
    for m in use_metrics:
        for mode_val in work[mode_col].dropna().unique():
            a, b = _groups(work, mode_col, mode_val, cat_col, m)  # a=Public, b=Private
            n1, n2 = len(a), len(b)
            if n1 < min_n or n2 < min_n:
                continue

            # effects
            g = hedges_g(a,b)
            u_stat, p_u = stats.mannwhitneyu(a,b, alternative="two-sided")
            delta = cliffs_delta_from_u(u_stat, n1, n2)

            # bootstrap CIs
            diff_mean, ci_mean = bootstrap_diff(a,b, fn=np.mean, n_boot=n_boot)
            diff_med,  ci_med  = bootstrap_diff(a,b, fn=np.median, n_boot=n_boot)

            rows.append({
                "metric": m, "mode": mode_val,
                "n_public": n1, "n_private": n2,
                "mean_public": float(np.mean(a)), "mean_private": float(np.mean(b)),
                "median_public": float(np.median(a)), "median_private": float(np.median(b)),
                "diff_mean (Priv−Pub)": diff_mean, "mean_CI95": ci_mean,
                "diff_median (Priv−Pub)": diff_med, "median_CI95": ci_med,
                "Hedges_g": g, "Cliffs_delta": delta
            })
    out = pd.DataFrame(rows).sort_values(["metric","mode"]).reset_index(drop=True)
    display(out)

    # quick bar plot of mean differences with 95% CI
    if not out.empty:
        plt.figure()
        labels = [f"{m} | {md}" for m, md in zip(out["metric"], out["mode"])]
        x = np.arange(len(out))
        y = out["diff_mean (Priv−Pub)"].values
        yerr = np.vstack([y - out["mean_CI95"].apply(lambda t: t[0]).values,
                          out["mean_CI95"].apply(lambda t: t[1]).values - y])
        plt.bar(x, y)
        plt.errorbar(x, y, yerr=yerr, fmt='none')
        plt.xticks(x, labels, rotation=45, ha='right')
        plt.ylabel("Mean difference (Private − Public)")
        plt.title("Private vs Public within Mode — effect size view")
        plt.tight_layout()
        plt.show()

    return out

# Run (edit metrics if you want):
effects_summary = effect_sizes_and_CIs(
    df,
    metrics=("Avg Fares Per Trip FY","Avg Trip Length FY"),
    mode_col_candidates=("3 Mode","Mode"),
    min_n=5, n_boot=4000
)

# Build stars from adjusted p-values (prefer U-test; fallback to t-test)
def p_to_stars(p):
    if p < 0.001: return '***'
    if p < 0.01:  return '**'
    if p < 0.05:  return '*'
    return 'ns'

# Merge effects with p-values (U-test first)
anno = effects_summary.copy()
key = ["metric","mode"]

if "p_adj" in u_fdr.columns:
    pmap_u = u_fdr[[*key,"p_adj"]].rename(columns={"p_adj":"p_adj_u"})
    anno = anno.merge(pmap_u, on=key, how="left")
if "p_adj" in t_fdr.columns:
    pmap_t = t_fdr[[*key,"p_adj"]].rename(columns={"p_adj":"p_adj_t"})
    anno = anno.merge(pmap_t, on=key, how="left")

# choose best available p
anno["p_use"] = anno["p_adj_u"].fillna(anno["p_adj_t"])
anno["stars"] = anno["p_use"].apply(lambda x: p_to_stars(x) if pd.notna(x) else "ns")

# Plot again with stars
import matplotlib.pyplot as plt
import numpy as np

labels = [f"{m} | {md}" for m, md in zip(anno["metric"], anno["mode"])]
x = np.arange(len(anno))
y = anno["diff_mean (Priv−Pub)"].values
ci_lo = anno["mean_CI95"].apply(lambda t: t[0]).values
ci_hi = anno["mean_CI95"].apply(lambda t: t[1]).values
yerr = np.vstack([y - ci_lo, ci_hi - y])

plt.figure()
plt.bar(x, y)
plt.errorbar(x, y, yerr=yerr, fmt='none')
for xi, yi, s in zip(x, y, anno["stars"]):
    plt.text(xi, yi + (0.03*max(abs(y))+1), s, ha='center', va='bottom', fontsize=10)
plt.xticks(x, labels, rotation=45, ha='right')
plt.ylabel("Mean difference (Private − Public)")
plt.title("Private vs Public within Mode — effect size view (with significance)")
plt.tight_layout()

# Save outputs
import os
os.makedirs("outputs", exist_ok=True)
plt.savefig("outputs/private_public_effects.png", dpi=200, bbox_inches="tight")
anno.to_csv("outputs/private_public_effects_table.csv", index=False)
print("saved: outputs/private_public_effects.png")
print("saved: outputs/private_public_effects_table.csv")

# ==== Two-way ANOVA with interaction (Public/Private × Mode) — full drop-in cell ====

# Map: Organization Type → Public/Private (used if "Org Category" is missing)
_ORG_MAP = {
    "Private-For-Profit Corporation": "Private",
    "Private-Non-Profit Corporation": "Private",
    "Private Provider Reporting on Behalf of a Public Entity": "Private",
    "City, County or Local Government Unit or Department of Transportation": "Public",
    "Independent Public Agency or Authority of Transit Service": "Public",
    "MPO, COG or Other Planning Agency": "Public",
    "Other Publicly-Owned or Privately Chartered Corporation": "Public",
    "State Government Unit or Department of Transportation": "Public",
    "Subsidiary Unit of a Transit Agency, Reporting Separately": "Public",
    "University": "Public",
}

def _clean_numeric_series(s):
    """Coerce strings with commas/currency/%/(negatives) to numeric."""
    import pandas as pd
    s = (s.astype(str)
          .str.replace(r'[,\s]', '', regex=True)
          .str.replace(r'[$€₪%]', '', regex=True)
          .str.replace('−', '-', regex=False)                     # unicode minus
          .str.replace(r'\(([^)]+)\)', r'-\1', regex=True))       # (123) -> -123
    return pd.to_numeric(s, errors='coerce')

def _ensure_pp_and_mode(df, org_cands=("Org Category","Organization Type"), mode_cands=("3 Mode","Mode")):
    """Return (work_df, org_col, mode_col) where org_col has Public/Private."""
    import pandas as pd
    work = df.copy()

    # org column
    if "Org Category" in work.columns:
        org_col = "Org Category"
    elif "Organization Type" in work.columns:
        org_col = "Org Category (derived)"
        work[org_col] = work["Organization Type"].map(_ORG_MAP)
    else:
        raise KeyError("Need 'Org Category' or 'Organization Type' column.")

    # mode column
    mode_col = next((c for c in mode_cands if c in work.columns), None)
    if mode_col is None:
        raise KeyError(f"No mode column found among: {mode_cands}")

    # keep only Public/Private and non-null
    work = work[work[org_col].isin(["Public","Private"])]
    work = work.dropna(subset=[org_col, mode_col])

    return work, org_col, mode_col

def _two_way_one_metric(df, y_col, org_col, mode_col, alpha=0.05):
    """
    Run two-way ANOVA: Q(y) ~ C(Q(org))*C(Q(mode))
    - Coerces y to numeric
    - Returns (anova_table_with_metric_col, dict of partial eta^2 per term)
    """
    import numpy as np, pandas as pd
    import statsmodels.formula.api as smf
    from statsmodels.stats.anova import anova_lm

    data = df[[y_col, org_col, mode_col]].copy()
    data[y_col] = _clean_numeric_series(data[y_col])
    data = data.dropna(subset=[y_col, org_col, mode_col])

    if data[org_col].nunique() < 2 or data[mode_col].nunique() < 2:
        print(f"Skip {y_col}: not enough levels in {org_col}/{mode_col}.")
        return None, None

    # NOTE: Q('col') is a patsy builtin; no import needed
    formula = f"Q('{y_col}') ~ C(Q('{org_col}')) * C(Q('{mode_col}'))"
    model = smf.ols(formula, data=data).fit()
    aov = anova_lm(model, typ=2)

    # partial eta^2 = SS_effect / (SS_effect + SS_error)
    ss_res = float(aov.loc["Residual","sum_sq"])
    effects = {}
    for term in aov.index:
        if term == "Residual":
            continue
        ss = float(aov.loc[term,"sum_sq"])
        effects[term] = ss / (ss + ss_res) if (ss + ss_res) > 0 else np.nan

    print(f"\n=== Two-way ANOVA — {y_col} ~ {org_col} * {mode_col} ===")
    display(aov)
    print("partial eta^2:", {k: (None if pd.isna(v) else round(v,3)) for k,v in effects.items()})

    return aov.assign(metric=y_col), effects

def two_way_anova_batch(df, metrics=("Avg Fares Per Trip FY","Avg Trip Length FY"), alpha=0.05):
    """
    Batch runner: iterates metrics, runs two-way ANOVA with interaction,
    prints each table, and returns a combined summary with p-values + partial eta^2.
    Also saves CSV to outputs/two_way_anova_summary.csv
    """
    import pandas as pd, os

    work, org_col, mode_col = _ensure_pp_and_mode(df)
    rows = []
    for y in metrics:
        if y not in work.columns:
            print(f"Skip (missing column): {y}")
            continue
        aov, eff = _two_way_one_metric(work, y, org_col, mode_col, alpha=alpha)
        if aov is None:
            continue
        for term in aov.index:
            if term == "Residual":
                continue
            rows.append({
                "metric": y,
                "term": term,
                "p_value": float(aov.loc[term, "PR(>F)"]),
                "partial_eta2": float(eff.get(term, float("nan")))
            })

    if rows:
        summary = pd.DataFrame(rows).sort_values(["metric","term"]).reset_index(drop=True)
        print("\n=== Summary across metrics (p-values + partial η²) ===")
        display(summary)
        os.makedirs("outputs", exist_ok=True)
        summary.to_csv("outputs/two_way_anova_summary.csv", index=False)
        print("saved: outputs/two_way_anova_summary.csv")
        return summary
    else:
        print("No valid results.")
        return None

# ---- Example run (edit metrics if you like) ----
two_way_summary = two_way_anova_batch(
    df,
    metrics=("Avg Fares Per Trip FY", "Avg Trip Length FY"),
    alpha=0.05
)

# Robust interaction plot (no warnings), with BH-FDR stars and export

import numpy as np, pandas as pd, matplotlib.pyplot as plt
from scipy import stats

ORG_MAP = {
    "Private-For-Profit Corporation": "Private",
    "Private-Non-Profit Corporation": "Private",
    "Private Provider Reporting on Behalf of a Public Entity": "Private",
    "City, County or Local Government Unit or Department of Transportation": "Public",
    "Independent Public Agency or Authority of Transit Service": "Public",
    "MPO, COG or Other Planning Agency": "Public",
    "Other Publicly-Owned or Privately Chartered Corporation": "Public",
    "State Government Unit or Department of Transportation": "Public",
    "Subsidiary Unit of a Transit Agency, Reporting Separately": "Public",
    "University": "Public",
}

def _clean_num(s):
    s = (s.astype(str)
         .str.replace(r'[,\s]', '', regex=True)
         .str.replace(r'[$€₪%]', '', regex=True)
         .str.replace('−', '-', regex=False)
         .str.replace(r'\(([^)]+)\)', r'-\1', regex=True))
    return pd.to_numeric(s, errors='coerce')

def _bh_fdr(p):
    p = np.asarray(p, float); n = len(p)
    order = np.argsort(p); ranked = p[order]
    adj = np.empty(n); cm = 1.0
    for i in range(n-1, -1, -1):
        val = ranked[i]*n/(i+1); cm = min(cm, val); adj[i] = min(cm, 1.0)
    out = np.empty(n); out[order] = adj; return out

def _boot_ci_mean(x, n_boot=4000, rng=np.random.default_rng(1)):
    x = np.asarray(x, float); x = x[~np.isnan(x)]
    if len(x)==0: return np.nan, (np.nan, np.nan)
    boots = [rng.choice(x, size=len(x), replace=True).mean() for _ in range(n_boot)]
    lo, hi = np.percentile(boots, [2.5, 97.5])
    return float(np.mean(boots)), (float(lo), float(hi))

def interaction_plot_metric_safe(df, y_col,
                                 org_cands=("Org Category","Organization Type"),
                                 mode_cands=("3 Mode","Mode"),
                                 alpha=0.05, min_n=5,
                                 save_tag=None):
    # choose org/mode columns and coerce Y to numeric
    if "Org Category" in df.columns:
        org = "Org Category"; work = df.copy()
    elif "Organization Type" in df.columns:
        org = "Org Category (derived)"; work = df.copy(); work[org] = work["Organization Type"].map(ORG_MAP)
    else:
        raise KeyError("Need 'Org Category' or 'Organization Type'.")
    mode = next((c for c in mode_cands if c in work.columns), None)
    if mode is None: raise KeyError(f"No mode column among: {mode_cands}")

    work = work[work[org].isin(["Public","Private"])].dropna(subset=[org,mode])
    work[y_col] = _clean_num(work[y_col])
    work = work.dropna(subset=[y_col])

    modes = list(work[mode].dropna().unique())

    rows = []
    for m in modes:
        g = work[work[mode]==m]
        a = g.loc[g[org]=="Public", y_col].dropna().to_numpy()
        b = g.loc[g[org]=="Private", y_col].dropna().to_numpy()
        if len(a) < min_n or len(b) < min_n:
            continue  # skip modes without enough data in BOTH groups
        mean_a, ci_a = _boot_ci_mean(a); mean_b, ci_b = _boot_ci_mean(b)
        p_u = stats.mannwhitneyu(a, b, alternative="two-sided").pvalue
        rows.append({
            "mode": m, "n_public": len(a), "n_private": len(b),
            "mean_public": mean_a, "ci_public": ci_a,
            "mean_private": mean_b, "ci_private": ci_b,
            "p_raw": p_u
        })

    out = pd.DataFrame(rows)
    if out.empty:
        print("No mode had enough data in both groups.");
        return out

    out["p_adj"] = _bh_fdr(out["p_raw"].values)
    out["stars"] = out["p_adj"].apply(lambda p: '***' if p<0.001 else '**' if p<0.01 else '*' if p<0.05 else 'ns')
    display(out.sort_values("p_adj"))

    # plot
    x = np.arange(len(out))
    y_pub  = out["mean_public"].values
    y_priv = out["mean_private"].values
    err_pub  = np.vstack([y_pub  - out["ci_public"].apply(lambda t: t[0]).values,
                          out["ci_public"].apply(lambda t: t[1]).values - y_pub ])
    err_priv = np.vstack([y_priv - out["ci_private"].apply(lambda t: t[0]).values,
                          out["ci_private"].apply(lambda t: t[1]).values - y_priv])

    plt.figure()
    plt.errorbar(x, y_pub,  yerr=err_pub,  fmt='-o', label="Public")
    plt.errorbar(x, y_priv, yerr=err_priv, fmt='-o', label="Private")
    for xi, (ya,yb,star) in enumerate(zip(y_pub, y_priv, out["stars"])):
        vals = np.array([ya, yb], dtype=float)
        if np.isfinite(vals).any():
            y_top = np.nanmax(vals[np.isfinite(vals)])
            plt.text(xi, y_top + 0.05*(np.nanmax(np.r_[y_pub,y_priv]) - np.nanmin(np.r_[y_pub,y_priv]) + 1),
                     star, ha='center', va='bottom', fontsize=10)

    plt.xticks(x, out["mode"].tolist())
    plt.xlabel(mode); plt.ylabel(y_col)
    plt.title(f"Interaction: Public vs Private across {mode} — {y_col}")
    plt.legend(); plt.tight_layout()

    # save
    import os
    os.makedirs("outputs", exist_ok=True)
    tag = save_tag or y_col.replace(" ", "_")
    fig_path = f"outputs/interaction_{tag}.png"
    csv_path = f"outputs/interaction_{tag}_table.csv"
    plt.savefig(fig_path, dpi=200, bbox_inches="tight")
    out.to_csv(csv_path, index=False)
    print(f"saved: {fig_path}\nsaved: {csv_path}")
    plt.show()

    return out

# Run safely for both metrics (no warnings, with exports)
interaction_plot_metric_safe(df, "Avg Fares Per Trip FY", save_tag="avg_fares_per_trip_fy")
interaction_plot_metric_safe(df, "Avg Trip Length FY", save_tag="avg_trip_length_fy")

from pathlib import Path
sorted(str(p) for p in Path("outputs").glob("*"))

